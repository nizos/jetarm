{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "third-divide",
   "metadata": {},
   "source": [
    "# JETARM - Live Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ordered-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-courage",
   "metadata": {},
   "source": [
    "Load the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "macro-freeware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load('model_trt.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-projection",
   "metadata": {},
   "source": [
    "Create the preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "overall-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, std)\n",
    "\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-grove",
   "metadata": {},
   "source": [
    "Start and display the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "superior-fishing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d9ce870b874abf940743ba9f49fc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00Câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import traitlets\n",
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "image = ipywidgets.Image(format='jpeg', width=224, height=224)\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(ipywidgets.VBox([image]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-valentine",
   "metadata": {},
   "source": [
    "Create the robot instance to drive the motors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "danish-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "from SCSCtrl import TTLServo\n",
    "import time\n",
    "\n",
    "robot = Robot()\n",
    "\n",
    "def stop(change):\n",
    "    robot.stop()\n",
    "\n",
    "def limitCtl(maxInput, minInput, rawInput):\n",
    "    if rawInput > maxInput:\n",
    "        limitBuffer = maxInput\n",
    "    elif rawInput < minInput:\n",
    "        limitBuffer = minInput\n",
    "    else:\n",
    "        limitBuffer = rawInput\n",
    "    return limitBuffer\n",
    "\n",
    "\n",
    "gripPos = -50\n",
    "gripMin = -90\n",
    "gripMax = -10\n",
    "\n",
    "xPos = 100\n",
    "xMin = 140\n",
    "xMax = 210\n",
    "\n",
    "yPos = 0\n",
    "yMin = -120\n",
    "yMax = 120\n",
    "\n",
    "def setGrip(newGripPos):\n",
    "    global gripPos\n",
    "    gripPos = limitCtl(gripMax, gripMin, newGripPos)\n",
    "    TTLServo.servoAngleCtrl(4, gripPos, 1, 500)\n",
    "\n",
    "def setXY(newXPos, newYPos):\n",
    "    global xPos, yPos\n",
    "    xPos = limitCtl(xMax, xMin, newXPos)\n",
    "    yPos = limitCtl(yMax, yMin, newYPos)\n",
    "    TTLServo.xyInput(xPos, yPos)\n",
    "\n",
    "def setY(newYPos):\n",
    "    global xPos, yPos\n",
    "    yPos = limitCtl(yMax, yMin, newYPos)\n",
    "    TTLServo.xyInput(xPos, yPos)\n",
    "\n",
    "# Set the start position\n",
    "# Turn the arm to the left\n",
    "TTLServo.servoAngleCtrl(1, 90, 1, 150)\n",
    "time.sleep(3)\n",
    "TTLServo.servoStop(1)\n",
    "\n",
    "# Extend the arm forward\n",
    "setXY(xPos, yPos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-disco",
   "metadata": {},
   "source": [
    "Create the update function which will pre-process the camera image, execute the nural network, and handle the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "apparent-black",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca56459400845cf84c7fb8fcffb3a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='224', width='224')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "prediction_widget = ipywidgets.widgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "\n",
    "def update(change):\n",
    "    global prediction_widget\n",
    "    new_image = change['new'] \n",
    "    preprocessed = preprocess(new_image)\n",
    "    output = model_trt(preprocessed).detach().cpu().numpy().flatten()\n",
    "\n",
    "    # Thumb coordinates\n",
    "    thumb_x = output[2 * 0]\n",
    "    thumb_y = output[2 * 0 + 1]\n",
    "    thumb_x = int(camera.width * (thumb_x / 2.0 + 0.5))\n",
    "    thumb_y = int(camera.height * (thumb_y / 2.0 + 0.5))\n",
    "\n",
    "    # Index coordinates\n",
    "    index_x = output[2 * 1]\n",
    "    index_y = output[2 * 1 + 1]\n",
    "    index_x = int(camera.width * (index_x / 2.0 + 0.5))\n",
    "    index_y = int(camera.height * (index_y / 2.0 + 0.5))\n",
    "\n",
    "    # Set grip width\n",
    "    grip_width = abs(thumb_x - index_x)\n",
    "    new_grip_val = (2*grip_width) - 110\n",
    "    new_grip_pos = limitCtl(gripMax, gripMin, new_grip_val)\n",
    "    setGrip(new_grip_pos)\n",
    "\n",
    "    # Set arm elivation\n",
    "    hand_min_x = min([thumb_x, index_x])\n",
    "    hand_max_x = max([thumb_x, index_x])\n",
    "    hand_min_y = min([thumb_y, index_y])\n",
    "    hand_max_y = max([thumb_y, index_y])\n",
    "\n",
    "    hand_y_center = sum([hand_min_y, hand_max_y]) / 2\n",
    "    new_hand_val = (0-hand_y_center) + 172\n",
    "    new_hand_pos = limitCtl(yMax, yMin, new_hand_val)\n",
    "    setY(new_hand_pos)\n",
    "\n",
    "    # Move robot wheels\n",
    "    if hand_min_x < 60 and hand_max_x < 164:\n",
    "        if hand_min_x < 24:\n",
    "            robot.forward(0.4)\n",
    "        else:\n",
    "            robot.forward(0.2)\n",
    "    elif hand_max_x > 164 and hand_min_x > 60:\n",
    "        if hand_min_x > 200:\n",
    "            robot.backward(0.4)\n",
    "        else:\n",
    "            robot.backward(0.2)\n",
    "    else:\n",
    "        robot.stop()\n",
    "\n",
    "    # Show hand tracking\n",
    "    prediction = new_image.copy()\n",
    "    prediction = cv2.circle(prediction, (thumb_x, thumb_y), 8, (255, 0, 0), 2)\n",
    "    prediction = cv2.circle(prediction, (index_x, index_y), 8, (255, 0, 0), 2)\n",
    "    cv2.rectangle(prediction, pt1=(48, 48), pt2=(176, 176), color=(0, 255, 255), thickness=1)\n",
    "    \n",
    "    # Show grip data\n",
    "    grip_info = \"W: \" + str(grip_width) + \", P: \" + str(new_grip_pos)\n",
    "    thumb_info = \"Th: \" + str(thumb_x) + \", \" + str(thumb_y)\n",
    "    index_info = \"In: \" + str(index_x) + \", \" + str(index_y)\n",
    "    cv2.putText(prediction, grip_info, (32, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (51, 255, 153), 1, cv2.LINE_AA)\n",
    "    cv2.putText(prediction, thumb_info, (32, 170), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (51, 255, 153), 1, cv2.LINE_AA)\n",
    "    cv2.putText(prediction, index_info, (32, 190), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (51, 255, 153), 1, cv2.LINE_AA)\n",
    "    \n",
    "    # Show elivation data\n",
    "    elivation_center_info = \"C: \" + str(hand_y_center)\n",
    "    elivation_pos_info = \"HP: \" + str(new_hand_pos)\n",
    "    cv2.putText(prediction, elivation_center_info, (32, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (51, 255, 153), 1, cv2.LINE_AA)\n",
    "    cv2.putText(prediction, elivation_pos_info, (32, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (51, 255, 153), 1, cv2.LINE_AA)\n",
    "    \n",
    "    # Show result\n",
    "    prediction_widget.value = bgr8_to_jpeg(prediction)\n",
    "\n",
    "# Display AI Camera feed with annotations\n",
    "display(prediction_widget)\n",
    "\n",
    "# Initialize\n",
    "update({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-container",
   "metadata": {},
   "source": [
    "Attach the camera to the nural network execution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "numerous-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.observe(update, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-enclosure",
   "metadata": {},
   "source": [
    "## End Demo\n",
    "\n",
    "Unattach the callback function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "magnetic-short",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "camera.unobserve(update, names='value')\n",
    "\n",
    "time.sleep(0.1)\n",
    "\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-radiation",
   "metadata": {},
   "source": [
    "Close the camera conneciton so that it can be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "productive-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-revelation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
